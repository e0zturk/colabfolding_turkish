{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e0zturk/colabfolding_turkish/blob/main/folding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAAu6AXynIBF"
      },
      "source": [
        "#**Google Colab**\n",
        "## ColabFOLD & AlphaFold\n",
        "\n",
        "[Resmi Repo: LocalColabFold](https://github.com/YoshitakaMo/localcolabfold) | [YÃ¶ntem: AlphaFold2](https://nature.com/articles/s41586-021-03819-2)\n",
        "\n",
        "Bu Ã§alÄ±ÅŸma defteri, **LocalColabFold** mimarisini kullanarak amino asit dizilerinden (Fasta) yÃ¼ksek doÄŸrulukta **3 boyutlu protein yapÄ±larÄ± (.pdb)** ve kompleksleri oluÅŸturmak iÃ§in tasarlanmÄ±ÅŸtÄ±r.\n",
        "\n",
        "**LocalColabFold Nedir?**\n",
        "Google DeepMind tarafÄ±ndan geliÅŸtirilen **AlphaFold2** algoritmasÄ±nÄ±n, daha hÄ±zlÄ± ve eriÅŸilebilir bir versiyonudur. **MMseqs2** algoritmasÄ± kullanÄ±larak oluÅŸturulan derin Ã‡oklu Dizi HizalamasÄ± (MSA) ve PDB veritabanÄ±ndaki homolog ÅŸablonlarÄ±n (templates) entegrasyonuna dayanÄ±r. Bu yÃ¶ntem, pLDDT skorunu maksimize ederek deneysel verilere en yakÄ±n topolojiyi sunar.\n",
        "\n",
        "**Ã–zellikler:**\n",
        "* **Tahmin algoritmasÄ±**: **MMseqs2**\n",
        "\n",
        "* **HÄ±z:** Notebook editÃ¶rÃ¼, T4 GPU Ã¼zerinde Ã§alÄ±ÅŸacak ÅŸekilde, 3 cycle optimizasyon dÃ¶ngÃ¼sÃ¼nde (varsayÄ±lan); 100 amino asitlik ortalama bir sekansÄ±n tam analizini (MSA + 5 Model + Relaxation) yaklaÅŸÄ±k 10 dakika sÃ¼rdÃ¼ÄŸÃ¼nÃ¼ yaptÄ±ÄŸÄ± deneme testlerinde doÄŸrulamÄ±ÅŸtÄ±r. HÄ±zÄ±nÄ±zÄ± arttÄ±rmak iÃ§in COLAB aboneliÄŸi & birim satÄ±n alma gerÃ§ekleÅŸtirebilirsiniz.\n",
        "\n",
        "* **Kompleks Modelleme:** VarsayÄ±lan olarak AlphaFold2_multimer_v3 motoru kullanÄ±lÄ±r. Bu sÃ¼rÃ¼m, protein-protein etkileÅŸim arayÃ¼zlerini (interface) Ã§Ã¶zmek iÃ§in Ã¶zel olarak eÄŸitilmiÅŸtir; dimer, trimer veya daha bÃ¼yÃ¼k oligomerik yapÄ±larÄ±n stokiyometrik analizini mÃ¼mkÃ¼n kÄ±lar."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â­•SÄ°STEM GEREKSÄ°NÄ°MLERÄ° :\n",
        "* 17+ GB DRÄ°VE DEPOLAMA ALANI\n",
        "\n",
        "---\n",
        "Colabfold programÄ±nÄ±n dosya paketleri yaklaÅŸÄ±k 16.3 GB yer kaplamaktadÄ±r. GiriÅŸ yapÄ±lan Google Drive hesabÄ±nÄ±zÄ±n yeterli depolama alanÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol ediniz. Aksi takdirde kurulum gerÃ§ekleÅŸmez."
      ],
      "metadata": {
        "id": "pOEyJvhFiFOS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PwW5AO7p6ZA"
      },
      "source": [
        "## **HazÄ±rlÄ±k ve Filtrasyon**\n",
        "\n",
        "\n",
        "Ã‡alÄ±ÅŸmanÄ±n saÄŸlÄ±klÄ± yÃ¼rÃ¼tÃ¼lmesi adÄ±na Google Colab sizden; **verilerinizi** tanÄ±mak, iÅŸlemek, Ã§alÄ±ÅŸtÄ±rmak ve sonuÃ§larÄ± kaydetmek amacÄ±yla `Google Drive` hesabÄ±na eriÅŸim izni isteyecektir. **LÃ¼tfen eriÅŸim izinlerini onaylayÄ±nÄ±z.**\n",
        "\n",
        "**Ä°lk defa Ã§alÄ±ÅŸtÄ±rmanÄ±z halinde Drive dizinine otomatik `Folding/data` dosyalarÄ± tanÄ±mlanacaktÄ±r. Ä°lk Ã§alÄ±ÅŸtÄ±rÄ±lmadan alÄ±nan HATA'yÄ± tekrar edilmemesi adÄ±na Ã§alÄ±ÅŸma dosyasÄ±nÄ± `raw_data` .fasta formatÄ±nda yÃ¼klemeniz gerekmektedir.**\n",
        "\n",
        "**`data/` klasÃ¶rÃ¼ne yÃ¼klediÄŸiniz `.fasta` dosyasÄ±nÄ±n tam adÄ±nÄ± hÃ¼crede girdi olarak verilmesi elzemdir.**\n",
        "\n",
        "**`raw_data = \"xxxx.fasta\"`**\n",
        "\n",
        "\n",
        "`raw_data` olarak atadÄ±ÄŸÄ±nÄ±z deÄŸiÅŸken dosyasÄ± iÃ§erisinde bulunan bazÄ± bozuk dosyalar ve tekrar eden diziler filtreleme aÅŸamasÄ±ndan geÃ§er.\n",
        "\n",
        "---\n",
        "### BÄ°LGÄ° â„¹ï¸\n",
        "- HÃ¼creyi Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra filtrelenmiÅŸ (`.fasta`) dosyasÄ±nÄ±n ne kadar saÄŸlam sekans iÃ§erdiÄŸi bilgisi Ã§Ä±ktÄ±da `SaÄŸlam sekans sayÄ±sÄ±:` ÅŸeklinde paylaÅŸÄ±lmÄ±ÅŸtÄ±r. Bir sonraki bÃ¶lÃ¼mde kullanmak Ã¼zere not etmeyi **UNUTMAYINIZ**\n",
        "* HÃ¼cre, **`raw_data`** Ã¶nceden yÃ¼klenmiÅŸ filtresiz dosyasÄ±nÄ± (`xxx.fasta`) not defteri Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda `MyDrive/Folding` klasÃ¶rÃ¼nden silecektir. Onun yerine filtrelenmiÅŸ, saÄŸlam yapÄ±lar iÃ§eren aynÄ± isimde (`xxx.fasta`) dosyasÄ± Ã¼retecektir. Ä°ÅŸlenmemiÅŸ `.fasta` dosyanÄ±zÄ± local (kiÅŸisel bilgisayarÄ±nÄ±z) dosyalarda yedeklediÄŸinizden **emin** olun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iXbdg_yqGFI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "#___________Ã‡ALIÅMA ALANI__________\n",
        "raw_data = \"HPV.fasta\"  # Ã¶rnek dosya (kendiniz yÃ¼klemelisiniz, deÄŸiÅŸkene atamalÄ±sÄ±nÄ±z)\n",
        "Work_on_dir = \"/content/drive/MyDrive/Folding\"\n",
        "#__________________________________\n",
        "\n",
        "print(\"ğŸ”Œ Google Drive baÄŸlanÄ±yor...\")\n",
        "try:\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(\"âŒ Drive baÄŸlantÄ±sÄ± kurulamadÄ±. LÃ¼tfen sol taraftaki 'Dosyalar' simgesinden Drive simgesine tÄ±klayarak manuel baÄŸlamayÄ± deneyin.\")\n",
        "    raise e\n",
        "\n",
        "data_dir = os.path.join(Work_on_dir, \"data\")\n",
        "folders_created = False\n",
        "\n",
        "if not os.path.exists(Work_on_dir):\n",
        "    os.makedirs(Work_on_dir, exist_ok=True)\n",
        "    folders_created = True\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    folders_created = True\n",
        "\n",
        "if folders_created:\n",
        "    print(f\"â„¹ï¸ '{Work_on_dir}' ve 'data/' klasÃ¶rleriniz otomatik olarak oluÅŸturulmuÅŸtur.\")\n",
        "\n",
        "file_path = os.path.join(data_dir, raw_data)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"\\nâš ï¸ LÃ¼tfen data klasÃ¶rÃ¼ne ({data_dir}) Ã§alÄ±ÅŸmak istediÄŸiniz '{raw_data}' dosyasÄ±nÄ± yÃ¼kleyiniz.\")\n",
        "else:\n",
        "    print(f\"\\nâœ”ï¸ BaÄŸlanma baÅŸarÄ±lÄ±. Ä°ÅŸlenecek dosya doÄŸrulandÄ±: {file_path}\\n\")\n",
        "\n",
        "    seen_sequences = set()\n",
        "    clean_sequences = []\n",
        "    removed_x_count = 0\n",
        "    removed_dup_count = 0\n",
        "\n",
        "    current_header = None\n",
        "    current_seq_lines = []\n",
        "\n",
        "    def process_sequence(header, seq_lines):\n",
        "        global removed_x_count, removed_dup_count\n",
        "        full_seq = \"\".join(seq_lines).strip().upper()\n",
        "        if not full_seq: return\n",
        "        if full_seq in seen_sequences:\n",
        "            removed_dup_count += 1\n",
        "            return\n",
        "        if full_seq.count(\"X\") > 3:\n",
        "            removed_x_count += 1\n",
        "            return\n",
        "        seen_sequences.add(full_seq)\n",
        "        clean_sequences.append((header, full_seq))\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            if line.startswith(\">\"):\n",
        "                if current_header:\n",
        "                    process_sequence(current_header, current_seq_lines)\n",
        "                current_header = line\n",
        "                current_seq_lines = []\n",
        "            else:\n",
        "                current_seq_lines.append(line)\n",
        "        if current_header:\n",
        "            process_sequence(current_header, current_seq_lines)\n",
        "\n",
        "    total_clean = len(clean_sequences)\n",
        "    if total_clean > 0:\n",
        "        with open(file_path, 'w') as f:\n",
        "            for header, seq in clean_sequences:\n",
        "                f.write(f\"{header}\\n{seq}\\n\")\n",
        "\n",
        "        total_raw = total_clean + removed_dup_count + removed_x_count\n",
        "        average = sum(len(seq) for _, seq in clean_sequences) / total_clean\n",
        "\n",
        "        print(f\"ğŸ“Š Ä°statistikler:\")\n",
        "        print(f\"   - Ham Veri       : {total_raw}\")\n",
        "        print(f\"   - Tekrar Eden    : {removed_dup_count}\")\n",
        "        print(f\"   - Bozuk (X > 3)  : {removed_x_count}\")\n",
        "        print(f\"   - Eliminasyon %  : %{((1-(total_clean/total_raw))*100):.2f}\")\n",
        "        print(f\"\\nâœ”ï¸ Filtreleme baÅŸarÄ±yla gerÃ§ekleÅŸti.\")\n",
        "        print(f\"SaÄŸlam sekans sayÄ±sÄ±: {total_clean}\")\n",
        "        print(f\"Ortalama Sekans UzunluÄŸu: {average:.2f}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ HATA: Filtreleme sonucu elde kalan veri yok.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jA4Mqky3x17"
      },
      "source": [
        "## **Veri iÅŸleme**\n",
        "**Ä°ÅŸlenmiÅŸ veriler Ã¼zerinde gerekli tÃ¼m ayarlamalarÄ±n gerÃ§ekleÅŸtiÄŸi kÄ±sÄ±mdÄ±r.**\n",
        "###**SÄ°STEM AYARLARI:**\n",
        "1. parÃ§alama stratejisi ayarlarÄ±\n",
        "-  mode : `aim` ya da `rate`\n",
        "- parÃ§a sayÄ±sÄ±: `split_param` (integer)\n",
        "2. RUN (Ã§alÄ±ÅŸtÄ±rma) ayarlarÄ±\n",
        "- Ã‡alÄ±ÅŸtÄ±rma modu : `monomer` ya da `multimerv3`\n",
        "- Ã‡alÄ±ÅŸtÄ±rÄ±lmak istenen parÃ§a sayÄ±sÄ± : `kota` (integer)\n",
        "- KaldÄ±ÄŸÄ±nÄ±z yerden devam etme: `take_up` = (boolen) , `resume_as` = (integer)\n",
        "- Sunucunun optimizasyon dÃ¶ngÃ¼sÃ¼: `cycle`(integer)\n",
        "\n",
        "---\n",
        "#####NOTLAR:\n",
        "* Ä°ÅŸlenmiÅŸ dosyanÄ±zÄ±n kaÃ§ tane sekans iÃ§erdiÄŸi bilgisini `HazÄ±rlÄ±k` bÃ¶lÃ¼mÃ¼ne giderek Ã¶ÄŸrenebilirsiniz.\n",
        "\n",
        "* Fasta formatÄ±ndaki dosyanÄ±zÄ±n sekans sayÄ±sÄ±nÄ±n uzun olmasÄ± programÄ±n tamamlanma sÃ¼resini uzatabilir. Bu sÃ¼reyi en aza indirmek iÃ§in `.fasta` dosyanÄ±z parÃ§alara ayÄ±rmanÄ±z Ã¶nemlidir.\n",
        "\n",
        "* ParÃ§alanmÄ±ÅŸ dosyalar _`Folding/*fasta_parts`_ dizinine kaydedilir.\n",
        "\n",
        "* Yeni bir iÅŸleme baÅŸlanÄ±ldÄ±ÄŸÄ±nda Ã¶nceki Ã§alÄ±ÅŸmalarda oluÅŸturulmuÅŸ _`*_fasta-parts`_ klasÃ¶rÃ¼ **silinmektedir**.\n",
        "\n",
        "---\n",
        "##### BÃ¶lme stratejisi\n",
        "* `rate` : ParÃ§alamak iÃ§in orantÄ±sal metod kullanÄ±r. Ortalama olarak istenilen `split_param` parÃ§a sayÄ±sÄ± kadar parÃ§aya bÃ¶lÃ¼nÃ¼r. Ã–rneÄŸin \"rate\", 5 olarak deÄŸiÅŸken atamasÄ± yapmak '**Toplam 5 parÃ§a olacak ÅŸekilde bÃ¶l**' anlamÄ±na gelir.\n",
        "\n",
        "* `aim`: ParÃ§alarÄ±n hedef sekans sayÄ±sÄ±na gÃ¶re dosyanÄ±zÄ± bÃ¶ler. `split_param` olarak yazdÄ±ÄŸÄ±nÄ±z uzunlukta eÅŸit parÃ§alar Ã¼retir. Son parÃ§a eÅŸit daÄŸÄ±lan sekans parÃ§alarÄ±ndan eksik kalabilir. Ã–rneÄŸin `\"aim\"` , `30` atamasÄ± yapmak '**ParÃ§alar 30 sekans iÃ§erecek ÅŸekilde bÃ¶l**' anlamÄ±na gelir.\n",
        "\n",
        "---\n",
        "##### RUN (Ã§alÄ±ÅŸtÄ±rma) stratejisi\n",
        "* Ã‡alÄ±ÅŸtÄ±rma modunu dimer, trimer yapÄ±larÄ± iÃ§in varsayÄ±lan `multimerv3` da tutmanÄ±zÄ± Ã¶neriyoruz.\n",
        "* `kota`: ParÃ§alanan `.fasta` dosyalarÄ±nÄ±  RUN (Ã§alÄ±ÅŸtÄ±rma) sunucusuna sÄ±rasÄ±yla dÃ¶ngÃ¼ halinde Ã§alÄ±ÅŸtÄ±rmanÄ±zÄ± saÄŸlar. Tavsiye edilen `1`'dir.\n",
        "* `take_up` kaldÄ±ÄŸÄ±nÄ±z yerden devam ettirmek iÃ§in `True` (aÃ§Ä±k) boolen deÄŸiÅŸkeni, varsayÄ±lan olarak kapatmak iÃ§in `False` boolen kullanabilirsiniz. `resume_as` hangi parÃ§adan devam etmek istediÄŸinizi belirtmek iÃ§indir. (Ã¶rn: `xxx_part3.fasta`) -> `resume_as = 3`\n",
        "* `cycle` deÄŸiÅŸkeni colabfold'un `.pdb` dosya taramasÄ±nda yaptÄ±ÄŸÄ± optimizasyon dÃ¶ngÃ¼ sayÄ±sÄ±dÄ±r. ProgramÄ±n hÄ±zÄ±nÄ± etkileyebilir. HÄ±z sÄ±ralamasÄ± : 1,3,5,6,10,12. Tavsiye edilen dengeli tarama '3' olarak ayarlanmÄ±ÅŸtÄ±r.\n",
        "\n",
        "\n",
        "---\n",
        "#####âš ï¸UYARI : Ã‡alÄ±ÅŸtÄ±rmadan Ã¶nce `--- KULLANICI AYARLARI ---` kÄ±smÄ±nÄ± kendi Ã§alÄ±ÅŸmanÄ±za gÃ¶re **revize** ediniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLnhLGe-6V4q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "# --- KULLANICI AYARLARI (SÄ°STEM AYARLARI) ---\n",
        "mode = \"aim\"  #rate\n",
        "split_param = 1\n",
        "run_mode = \"monomerv2\" #monomerv2\n",
        "kota = 1  # oluÅŸan parÃ§alarÄ± {kota} kere dÃ¶ngÃ¼yle Ã§alÄ±ÅŸtÄ±rÄ±r.\n",
        "take_up = False #False\n",
        "resume_as = 6  #part{i} ve sonrasÄ±\n",
        "cycle = 1 # hÄ±z sÄ±ralamasÄ±: 1,3,5,6,10,12\n",
        "# --------------------------------------------\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError\n",
        "    print(f\"Sistem HazÄ±r. Ä°ÅŸlenecek Dosya: {os.path.basename(file_path)}\\n\")\n",
        "    print(f\"\\nâš™ï¸Ayarlar: Mod={mode}, Param={split_param}, RunMod={run_mode}, Kota={kota}\\n\\\n",
        "                Cycling={cycle},TakeUp={take_up}, ResumeAs={resume_as}\\n\")\n",
        "except (NameError, FileNotFoundError):\n",
        "    print(\"âŒ HATA: 'HazÄ±rlÄ±k ve Filtrasyon' bÃ¶lÃ¼mÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±lmamÄ±ÅŸ veya dosya bulunamadÄ±.\")\n",
        "    print(\"LÃ¼tfen Ã¶nce HazÄ±rlÄ±k bÃ¶lÃ¼mÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±n.\")\n",
        "else:\n",
        "    # --- TEMÄ°ZLÄ°K ---\n",
        "    print(\"ğŸ§¹'*_fasta_parts' klasÃ¶rleri taranÄ±yor...\")\n",
        "    for item in os.listdir(Work_on_dir):\n",
        "        item_path = os.path.join(Work_on_dir, item)\n",
        "        if os.path.isdir(item_path) and item.endswith(\"_fasta_parts\"):\n",
        "            try:\n",
        "                shutil.rmtree(item_path)\n",
        "                print(f\"\\nSilme iÅŸlemi baÅŸarÄ±lÄ±. yeniden boÅŸ dizin oluÅŸturuluyor.\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Silinemedi: {item} -> {e}\")\n",
        "    # ----------------\n",
        "\n",
        "    abs_path = os.path.abspath(file_path)\n",
        "    base_name = os.path.splitext(os.path.basename(abs_path))[0]\n",
        "    output_dir = os.path.join(Work_on_dir, f\"{base_name}_fasta_parts\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # ------------------------\n",
        "\n",
        "    clean_sequences = []\n",
        "    current_header = None\n",
        "    current_seq_lines = []\n",
        "\n",
        "    with open(abs_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            if line.startswith(\">\"):\n",
        "                if current_header:\n",
        "                    full_seq = \"\".join(current_seq_lines).strip()\n",
        "                    if full_seq: clean_sequences.append((current_header, full_seq))\n",
        "                current_header = line\n",
        "                current_seq_lines = []\n",
        "            else:\n",
        "                current_seq_lines.append(line)\n",
        "        if current_header:\n",
        "            full_seq = \"\".join(current_seq_lines).strip()\n",
        "            if full_seq: clean_sequences.append((current_header, full_seq))\n",
        "\n",
        "    total_seqs = len(clean_sequences)\n",
        "    chunks = []\n",
        "\n",
        "    if total_seqs > 0:\n",
        "        if mode == \"rate\":\n",
        "            if split_param > 0:\n",
        "                chunk_size = math.ceil(total_seqs / split_param)\n",
        "                for i in range(0, total_seqs, chunk_size):\n",
        "                    chunks.append(clean_sequences[i:i + chunk_size])\n",
        "                print(f\"\\nâ„¹ï¸ Mod: RATE -> Toplam {total_seqs} sekans, ortalama {len(chunks)} dosyaya bÃ¶lÃ¼nÃ¼yor.\\n\")\n",
        "\n",
        "        elif mode == \"aim\":\n",
        "            if split_param > 0:\n",
        "                for i in range(0, total_seqs, split_param):\n",
        "                    chunks.append(clean_sequences[i:i + split_param])\n",
        "                print(f\"\\nâ„¹ï¸ Mod: AIM -> Her dosyada en fazla {split_param} sekans olacak.\\n\")\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            output_file = os.path.join(output_dir, f\"{base_name}_part{i + 1}.fasta\")\n",
        "            with open(output_file, 'w') as out_f:\n",
        "                for header, seq in chunk:\n",
        "                    out_f.write(f\"{header}\\n{seq}\\n\")\n",
        "\n",
        "        print(f\"\\nâœ… Ä°ÅLEM TAMAM: Toplam {len(chunks)} parÃ§a oluÅŸturuldu.\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ HATA: Dosya boÅŸ veya okunamadÄ±. LÃ¼tfen HazÄ±rlÄ±k bÃ¶lÃ¼mÃ¼nÃ¼ kontrol edin.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFjSzilGjyMn"
      },
      "source": [
        "##**LOCAL COLAB FOLD KONTROLÃœ**\n",
        "`localcolabfold` dosyalarÄ±nÄ±n colab notebook'unun iÃ§inde varlÄ±ÄŸÄ±nÄ± sorgulamaktadÄ±r. EÄŸer yoksa `Drive` baÄŸlantÄ±sÄ± Ã¼zerinden `MyDrive` iÃ§inde `colabfold_yedek.tar` adÄ±nda editÃ¶rÃ¼n sunduÄŸu yedekleme dosyasÄ± kontrol edilir.\n",
        "\n",
        "---\n",
        "######Not\n",
        "* `'LocalColabFold zaten yÃ¼klÃ¼'` ise lÃ¼tfen $PATH aÅŸamasÄ±na geÃ§iniz.\n",
        "* Aksi takdirde manuel kurulum baÅŸlatÄ±nÄ±z.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEIZUgfMll0-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "    Work_on_dir\n",
        "except NameError:\n",
        "    Work_on_dir = \"/content/drive/MyDrive/Folding\"\n",
        "\n",
        "install_path = \"/content/localcolabfold\"\n",
        "backup_path = os.path.join(Work_on_dir, \"colabfold_yedek.tar\")\n",
        "\n",
        "if os.path.exists(install_path):\n",
        "    print(\"âœ… DURUM: LocalColabFold zaten yÃ¼klÃ¼.\\n\")\n",
        "    print(\"Manuel kurulumu atlayÄ±p $PATH bÃ¶lÃ¼mÃ¼ne geÃ§ebilirsiniz.\")\n",
        "\n",
        "else:\n",
        "    print(\"Drive kontrol ediliyor...\\n\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    if os.path.exists(backup_path):\n",
        "        print(f\"ğŸ“¥ Drive yedeÄŸi bulundu: {backup_path}\\n\")\n",
        "        print(\"â³ Yedek dosyasÄ± ÅŸimdi aÃ§Ä±lÄ±yor (HÄ±zÄ±nÄ±za gÃ¶re 5-10 dk sÃ¼rebilir)...\")\n",
        "\n",
        "        !cp \"{backup_path}\" /content/\n",
        "        !tar -xf /content/colabfold_yedek.tar -C /content/\n",
        "        print(\"âœ… Yedek .tar dosyasÄ± notebook'a yÃ¼klendi.\")\n",
        "    else:\n",
        "        print(f\"âŒ '{backup_path}' yolunda yedek bulunamadÄ±.\")\n",
        "        print(\"LÃ¼tfen bir sonraki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rarak 'Manuel Kurulum' yapÄ±nÄ±z.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVBuBwaurQs"
      },
      "source": [
        "## **Manuel Kurulum - ColabFold**\n",
        "\n",
        "Bu bÃ¶lÃ¼m, protein modelleme iÃ§in gerekli olan AlphaFold2 motorunu ve temel baÄŸÄ±mlÄ±lÄ±klarÄ± sisteme sunucudan kurar.\n",
        "\n",
        "---\n",
        "#####**Sistemin Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±:**\n",
        "**Girdi:**\n",
        "* Amino asit dizilerini iÃ§eren metin dosyalarÄ± (.fasta).\n",
        "\n",
        "**Ã‡Ä±ktÄ±lar:**\n",
        "* **_relaxed_...pdb:** Fiziksel enerji minimizasyonu (Amber) uygulanmÄ±ÅŸ, atomik Ã§akÄ±ÅŸmalarÄ±n giderildiÄŸi nihai ve en kararlÄ± model dosyasÄ±.\n",
        "* **_unrelaxed_...pdb:** AlgoritmanÄ±n Ã¼rettiÄŸi, henÃ¼z fiziksel optimizasyon yapÄ±lmamÄ±ÅŸ ham model dosyasÄ±.\n",
        "* **.json DosyalarÄ±:** Modellerin gÃ¼venilirlik skorlarÄ± ve sayÄ±sal metrik verileri.\n",
        "* **.png Grafikleri:** YapÄ±sal gÃ¼venilirlik (pLDDT) ve hizalama hatasÄ± (PAE) analiz gÃ¶rselleri\n",
        "\n",
        "---\n",
        "####âš ï¸UyarÄ±\n",
        "* localcolabfold dosyalarÄ± kurulumu iÃ§in lÃ¼tfen iÅŸlemin tamamen bitmesini bekleyiniz.\\\n",
        "Bu iÅŸlem 10-15 dakika sÃ¼rebilir\n",
        "* EÄŸer ilk defa not defterini Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z, manuel kurulumundan sonra otomatize kurulum iÃ§in dosyalarÄ±n yedeÄŸi Ã§Ä±karÄ±lacaktÄ±r. (`colabfold_yedek.tar`)\\\n",
        "Bu iÅŸlem de 10-20 dakika daha zaman alabilir.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg8Gs6RYuaXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "install_path = \"/content/localcolabfold\"\n",
        "backup_path = os.path.join(Work_on_dir, \"colabfold_yedek.tar\")\n",
        "\n",
        "free_gb = shutil.disk_usage(\"/content\").free / (1024**3)\n",
        "if free_gb < 17:\n",
        "    raise SystemExit(f\"âŒ Disk alanÄ± yetersiz: {free_gb:.2f} GB\")\n",
        "\n",
        "force_reinstall = not os.path.exists(backup_path)\n",
        "\n",
        "if force_reinstall or not os.path.exists(install_path):\n",
        "    if os.path.exists(install_path):\n",
        "        !rm -rf {install_path}\n",
        "\n",
        "    print(\"ğŸŒ MANUEL KURULUM baÅŸlatÄ±lÄ±yor...\")\n",
        "    print(\"âš ï¸ YaklaÅŸÄ±k 17 GB indirme yapÄ±lacak. Bu iÅŸlem 10-15 dk sÃ¼rebilir...\\n\")\n",
        "    !curl -fsSL https://pixi.sh/install.sh | bash\n",
        "    !git clone https://github.com/YoshitakaMo/localcolabfold.git {install_path}\n",
        "\n",
        "    %cd {install_path}\n",
        "    !/root/.pixi/bin/pixi install\n",
        "\n",
        "    if os.path.exists(os.path.join(install_path, \".pixi\")):\n",
        "        print(\"â³ Paketleme iÅŸlemi baÅŸlÄ±yor...\")\n",
        "        %cd /content\n",
        "        !tar -cf colabfold_yedek.tar localcolabfold\n",
        "        os.makedirs(Work_on_dir, exist_ok=True)\n",
        "        !mv colabfold_yedek.tar \"{backup_path}\"\n",
        "        print(\"âœ… Ä°ÅLEM TAMAM: Pixi ile kurulum ve yedekleme baÅŸarÄ±lÄ±.\")\n",
        "    else:\n",
        "        raise SystemExit(\"âŒ HATA: Pixi ortamÄ± oluÅŸturulamadÄ±!\")\n",
        "else:\n",
        "    print(\"âœ… Sistem zaten yÃ¼klÃ¼.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgdttGmd1bZQ"
      },
      "source": [
        "##**PATH$**\n",
        "---\n",
        "Kurulan dosyalarÄ± sisteme tanÄ±tÄ±r ve AlphaFold2 motorunun Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± test eder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5CETc_EM2K3p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "bin_path = \"/content/localcolabfold/localcolabfold/colabfold-conda/bin\"\n",
        "\n",
        "if os.path.exists(bin_path):\n",
        "    if bin_path not in os.environ['PATH']:\n",
        "        os.environ['PATH'] = f\"{bin_path}:{os.environ['PATH']}\"\n",
        "        print(f\"ğŸ”— PATH ayarlandÄ±: {bin_path}\\n\")\n",
        "\n",
        "    os.environ['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "    print(\"Sistem Test Ediliyor...\")\n",
        "    !colabfold_batch --help | head -n 5\n",
        "\n",
        "    print(\"\\nâœ… SÄ°STEM BÄ°LEÅENLERÄ° VE Colabfold HAZIR.\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ KRÄ°TÄ°K HATA: Kurulum tamamlanmÄ±ÅŸ gÃ¶rÃ¼nmÃ¼yor.\")\n",
        "    print(\"LÃ¼tfen 'Manuel Kurulum' veya 'Yedek YÃ¼kleme' adÄ±mlarÄ±nÄ± kontrol edin.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX41E4FP_t_6"
      },
      "source": [
        "## Colab Fold **RUN** â–¶ï¸\n",
        "Sistem BileÅŸenlerinin ve Fasta dosyalarÄ±nÄ±zÄ±n hazÄ±r olduÄŸundan eminseniz `RUN` baÅŸlatabilirsiniz.\n",
        "\n",
        "---\n",
        "##### â„¹ï¸ BÄ°LGÄ°LENDÄ°RME:\n",
        "* Colabfold, **`Folding/...fasta_parts`** klasÃ¶rÃ¼ iÃ§indeki oluÅŸturulan dosyalarÄ± Ã§alÄ±ÅŸtÄ±rabilir.\n",
        "\n",
        "* Ã‡Ä±ktÄ±lar `colabfold_results` klasÃ¶rÃ¼ne kaydedilecektir.\n",
        "\n",
        "* **Ã–nceden** elde edilmiÅŸ eski sonuÃ§lar **silinmektedir** ve yeniden temiz bir ÅŸekilde `colabfold_results` klasÃ¶rÃ¼ne  Ã§Ä±ktÄ±lar yÃ¼klenecektir.\n",
        "\n",
        "---\n",
        "#####âš ï¸UYARIâš ï¸:\n",
        "**TÃ¼m notebook hÃ¼crelerinin sÄ±rasÄ±yla yÃ¼rÃ¼tÃ¼ldÃ¼ÄŸÃ¼ senaryoda** sistem otomatik olarak sonuÃ§larÄ± bilgisayarÄ±nÄ±zÄ±n `Downloads` klasÃ¶rÃ¼ne indirecektir. DiÄŸer hÃ¼crelerin Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±nÄ± Ã¶nemsemeden ham Ã§Ä±ktÄ±yÄ± almak istiyorsanÄ±z Drive dizininizdeki `Folding/colabfold_results` dosyasÄ±nÄ± (**hÃ¼creyi tekrar Ã§alÄ±ÅŸtÄ±rmadan**) kontrol ediniz.\n",
        "\n",
        "---\n",
        "\n",
        "####VarsayÄ±lan ayarlar\n",
        "- **`--templates`**:PDB veritabanÄ±ndaki benzer yapÄ±larÄ± \"iskelet\" olarak kullanÄ±r.\n",
        "- **`--pair-mode unpaired_paired`**: GeniÅŸ tarama modu.\n",
        "- **`--model-type alphafold2_multimer_v3`**: Dimer, trimer yapÄ±larÄ± keÅŸfeder\n",
        "- **`--num-recycle 3`**: DÃ¼ÅŸÃ¼k derecede optimizasyon dÃ¶ngÃ¼sÃ¼.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOseUlzl62lB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import glob\n",
        "import sys\n",
        "import subprocess\n",
        "import signal\n",
        "\n",
        "try:\n",
        "    Work_on_dir\n",
        "except NameError:\n",
        "    Work_on_dir = \"/content/drive/MyDrive/Folding\"\n",
        "\n",
        "clean_target_dir = os.path.join(Work_on_dir, \"colabfold_results\")\n",
        "if os.path.exists(clean_target_dir):\n",
        "    try:\n",
        "        shutil.rmtree(clean_target_dir)\n",
        "        print(f\"ğŸ§¹ Temizlik YapÄ±ldÄ±: '{clean_target_dir}' ve iÃ§eriÄŸi silindi.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Temizlik sÄ±rasÄ±nda hata: {e}\")\n",
        "\n",
        "if 'run_mode' in locals() and run_mode == \"multimerv3\":\n",
        "    model_type_cli = \"alphafold2_multimer_v3\"\n",
        "else:\n",
        "    model_type_cli = \"alphafold2_ptm\"\n",
        "\n",
        "try:\n",
        "    CALISMA_YOLU = Work_on_dir\n",
        "    base_name_auto = os.path.splitext(raw_data)[0]\n",
        "    parts_dir = os.path.join(CALISMA_YOLU, f\"{base_name_auto}_fasta_parts\")\n",
        "    output_dir = os.path.join(CALISMA_YOLU, \"colabfold_results\")\n",
        "\n",
        "    found_files = glob.glob(os.path.join(parts_dir, f\"{base_name_auto}_part*.fasta\"))\n",
        "    total_parts = len(found_files)\n",
        "\n",
        "    if total_parts == 0:\n",
        "        raise FileNotFoundError(f\"KlasÃ¶rde hiÃ§ fasta parÃ§asÄ± bulunamadÄ±: {parts_dir}\")\n",
        "\n",
        "    start_at = 1\n",
        "    should_clean = True\n",
        "\n",
        "    batch_quota = kota if 'kota' in locals() else total_parts\n",
        "\n",
        "    if 'take_up' in locals() and take_up and 'resume_as' in locals():\n",
        "        should_clean = False\n",
        "        start_at = resume_as\n",
        "        if start_at > total_parts:\n",
        "            print(f\"\\nâ›” HATA: Ä°stediÄŸiniz baÅŸlangÄ±Ã§ ({start_at}), toplam parÃ§adan ({total_parts}) bÃ¼yÃ¼k!\")\n",
        "\n",
        "    end_at = start_at + batch_quota - 1\n",
        "    if end_at > total_parts:\n",
        "        end_at = total_parts\n",
        "\n",
        "    if should_clean:\n",
        "        if os.path.exists(output_dir):\n",
        "            try:\n",
        "                shutil.rmtree(output_dir)\n",
        "                print(\"ğŸ§¹ Temizlik: Eski sonuÃ§lar silindi.\")\n",
        "            except: pass\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    else:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    original_fasta_path = os.path.join(CALISMA_YOLU, raw_data)\n",
        "    backup_fasta_path = os.path.join(output_dir, raw_data)\n",
        "\n",
        "    if os.path.exists(original_fasta_path):\n",
        "        shutil.copy2(original_fasta_path, backup_fasta_path)\n",
        "    else:\n",
        "        print(f\"âš ï¸ UYARI: Orijinal fasta dosyasÄ± bulunamadÄ±! ({raw_data})\")\n",
        "\n",
        "    print(f\"ğŸ“‚ Hedef: {parts_dir}\")\n",
        "    print(f\"ğŸ“‚ Ã‡Ä±ktÄ±: {output_dir}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    recycle_count = cycle if 'cycle' in locals() else 3\n",
        "    process_counter = 0\n",
        "\n",
        "    for i in range(start_at, end_at + 1):\n",
        "        process_counter += 1\n",
        "        part_filename = f\"{base_name_auto}_part{i}.fasta\"\n",
        "\n",
        "        input_file_path = f\"{parts_dir}/{part_filename}\"\n",
        "        log_path = f\"{output_dir}/{base_name_auto}_part{i}_log.txt\"\n",
        "\n",
        "        if os.path.exists(input_file_path):\n",
        "            print(f\"\\nğŸ”‚ Ä°ÅLEM: [{process_counter}/{batch_quota}] | DOSYA: [{i}/{total_parts}] ('{part_filename}') BaÅŸlatÄ±lÄ±yor...\\n\\n\")\n",
        "\n",
        "            current_config = {\n",
        "                \"raw_data\": raw_data,\n",
        "                \"model_type\": model_type_cli,\n",
        "                \"split_param\": split_param if 'split_param' in locals() else \"None\",\n",
        "                \"cycle\": recycle_count,\n",
        "                \"total_parts\": total_parts,\n",
        "                \"processed_part\": i,\n",
        "                \"kota\": kota if 'kota' in locals() else \"Hepsi\",\n",
        "                \"take_up\": take_up if 'take_up' in locals() else False,\n",
        "                \"resume_as\": resume_as if 'resume_as' in locals() else 1\n",
        "            }\n",
        "\n",
        "            with open(log_path, \"w\") as logfile:\n",
        "                json.dump(current_config, logfile)\n",
        "                logfile.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "                logfile.flush()\n",
        "\n",
        "                command = [\n",
        "                    \"colabfold_batch\",\n",
        "                    input_file_path,\n",
        "                    output_dir,\n",
        "                    \"--templates\",\n",
        "                    \"--amber\",\n",
        "                    \"--use-gpu-relax\",\n",
        "                    \"--num-recycle\", str(recycle_count),\n",
        "                    \"--pair-mode\", \"unpaired_paired\",\n",
        "                    \"--model-type\", model_type_cli\n",
        "                ]\n",
        "\n",
        "                env = os.environ.copy()\n",
        "                env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "\n",
        "                process = subprocess.Popen(\n",
        "                    command,\n",
        "                    stdout=subprocess.PIPE,\n",
        "                    stderr=subprocess.STDOUT,\n",
        "                    text=True,\n",
        "                    bufsize=1,\n",
        "                    env=env\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    for line in iter(process.stdout.readline, ''):\n",
        "                        print(line, end='')\n",
        "                        logfile.write(line)\n",
        "                        logfile.flush()\n",
        "\n",
        "                    process.wait()\n",
        "\n",
        "                    if process.returncode == 0:\n",
        "                        print(f\"âœ… TamamlandÄ±: {part_filename}\")\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Hata ile bitti: {part_filename} (Kod: {process.returncode})\")\n",
        "\n",
        "                except KeyboardInterrupt:\n",
        "                    print(f\"\\nâ›” Ä°ÅLEM KULLANICI TARAFINDAN KESÄ°LDÄ°! ({part_filename})\\n\")\n",
        "                    print(\"ğŸ›‘ Arka plan iÅŸlemleri fiziksel olarak durduruldu....\")\n",
        "                    process.kill()\n",
        "                    process.wait()\n",
        "                    sys.exit(1)\n",
        "\n",
        "        else:\n",
        "            print(f\"âš ï¸ Dosya bulunamadÄ±: {part_filename}\")\n",
        "\n",
        "    print(f\"\\nâœ…HEDEFLENEN ({process_counter}) ADET Ä°ÅLEM BÄ°TTÄ°! SonuÃ§lar: '{output_dir}'\")\n",
        "\n",
        "except SystemExit as se:\n",
        "    pass\n",
        "except Exception as e:\n",
        "    print(f\"âŒ BEKLENMEYEN HATA: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuU3Ls2TFIcA"
      },
      "source": [
        "### **KÃ–K DOSYASI TASNÄ°FÄ°** - (optional)\n",
        "\n",
        "**KÃ¶k dizini `colabfold_results` iÃ§inde karmaÅŸÄ±k dosyalarÄ± derleyip dÃ¼zenli bir tasnif yapÄ±lmasÄ±nÄ± saÄŸlar.**\n",
        "\n",
        "---\n",
        "### âš ï¸ UYARI :  **`RUN`** iÅŸlemi tamamlanmadan Ã§alÄ±ÅŸtÄ±rmayÄ±nÄ±z.\n",
        "\n",
        "---\n",
        "####Not:\n",
        "- Uzun sÃ¼ren `RUN` iÅŸleminden sonra oturumunuz sonlanabilir. Bu durumda **`Drive`** kÃ¶k dizine baÄŸlantÄ± kurabilir Ã¶nceki bÃ¶lÃ¼mleri tekrardan Ã§alÄ±ÅŸtÄ±rmaya gerek kalmadan kÃ¶k dizinde iÅŸlem yapÄ±labilirsiniz.\n",
        "\n",
        "- KÃ¶k dizinizin oluÅŸtuÄŸu ve **RUN** iÅŸleminin  bittiÄŸinden eminseniz aÅŸaÄŸÄ±daki  `activing_code` kutucuÄŸunu iÅŸaretleyerek devam edin.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHzQM0gkIzDb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "activing_code = True # @param {\"type\":\"boolean\"}\n",
        "\n",
        "if not activing_code:\n",
        "    print(\"Ã‡alÄ±ÅŸtÄ±rmak iÃ§in kutucuÄŸu iÅŸaretleyin.\\n\")\n",
        "    sys.exit()\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ”Œ Google Drive baÄŸlanÄ±yor...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Folding/colabfold_results\"\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "    print(f\"âŒ HATA: '{base_path}' yolu bulunamadÄ±.\")\n",
        "    sys.exit()\n",
        "\n",
        "for item in os.listdir(base_path):\n",
        "    item_path = os.path.join(base_path, item)\n",
        "\n",
        "    if os.path.isdir(item_path):\n",
        "        sub_files = os.listdir(item_path)\n",
        "        if \"part\" in item or \"_env\" in item or \"_pairgreedy\" in item:\n",
        "             for sub_file in sub_files:\n",
        "                src = os.path.join(item_path, sub_file)\n",
        "                dst = os.path.join(base_path, sub_file)\n",
        "                try:\n",
        "                    shutil.move(src, dst)\n",
        "                except: pass\n",
        "             try:\n",
        "                 shutil.rmtree(item_path)\n",
        "             except: pass\n",
        "\n",
        "\n",
        "files = [f for f in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, f))]\n",
        "moved_count = 0\n",
        "pattern = re.compile(r\"^(YP_\\d+\\.\\d+|[A-Z0-9]+\\.\\d+|[A-Z0-9]+)\")\n",
        "\n",
        "for filename in files:\n",
        "    if filename.endswith(\".txt\") or filename.endswith(\".fasta\") or filename.endswith(\".json\") or filename == \"cite.bibtex\" or filename.startswith(\".\"):\n",
        "        continue\n",
        "\n",
        "    match = pattern.match(filename)\n",
        "    if match:\n",
        "        file_id = match.group(0)\n",
        "        clean_id = file_id\n",
        "\n",
        "        if clean_id == filename or len(clean_id) < 3:\n",
        "            continue\n",
        "\n",
        "        target_folder = os.path.join(base_path, clean_id)\n",
        "        source_path = os.path.join(base_path, filename)\n",
        "        dest_path = os.path.join(target_folder, filename)\n",
        "\n",
        "        try:\n",
        "            os.makedirs(target_folder, exist_ok=True)\n",
        "            shutil.move(source_path, dest_path)\n",
        "            moved_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ TaÅŸÄ±ma hatasÄ±: {filename} -> {e}\")\n",
        "\n",
        "for item in os.listdir(base_path):\n",
        "    item_path = os.path.join(base_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        if item.endswith(\"_env\") or item.endswith(\"_pairgreedy\"):\n",
        "            try: shutil.rmtree(item_path)\n",
        "            except: pass\n",
        "        elif not os.listdir(item_path):\n",
        "            try: os.rmdir(item_path)\n",
        "            except: pass\n",
        "\n",
        "remaining_files = [f for f in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, f))]\n",
        "deleted_files = 0\n",
        "\n",
        "for filename in remaining_files:\n",
        "    if (filename.endswith(\".txt\") or\n",
        "        filename.endswith(\".fasta\") or\n",
        "        filename.endswith(\"config.json\") or\n",
        "        filename == \"cite.bibtex\" or\n",
        "        filename.startswith(\".\")):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(base_path, filename)\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        deleted_files += 1\n",
        "    except: pass\n",
        "\n",
        "print(f\"\\nâœ… Ä°ÅLEM TAMAMLANDI: {moved_count} dosya dÃ¼zenlendi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8HLp8oxBZxd"
      },
      "source": [
        "##### Tasnif iÅŸleminin doÄŸruluÄŸunu kontrolÃ¼ iÃ§in opsiyonel Ã§alÄ±ÅŸÄ±r."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Lke-hZ12Bj"
      },
      "outputs": [],
      "source": [
        "%cd  \"/content/drive/MyDrive/Folding/colabfold_results\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THAxH5JDgjIa"
      },
      "source": [
        "## SONUÃ‡LARI KAYDETME\n",
        "**TÃ¼m sonuÃ§larÄ± tek bir paket haline getirir ve bilgisayarÄ±nÄ±za indirir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE8uvW3Mgriy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import glob\n",
        "from google.colab import files\n",
        "\n",
        "results_dir = \"/content/drive/MyDrive/Folding/colabfold_results\"\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    for item in os.listdir(results_dir):\n",
        "        item_path = f\"{results_dir}/{item}\"\n",
        "        if os.path.isdir(item_path) and not os.listdir(item_path):\n",
        "            try:\n",
        "                os.rmdir(item_path)\n",
        "            except: pass\n",
        "\n",
        "if 'raw_data' not in locals():\n",
        "    print(\"âš ï¸ DeÄŸiÅŸkenler hafÄ±zada yok. Log dosyasÄ±ndan okunuyor...\")\n",
        "    log_files = glob.glob(f\"{results_dir}/*_part*_log.txt\")\n",
        "\n",
        "    if log_files:\n",
        "        latest_log = max(log_files, key=os.path.getmtime)\n",
        "        try:\n",
        "            with open(latest_log, 'r') as f:\n",
        "                first_line = f.readline().strip()\n",
        "                if first_line.startswith(\"{\") and first_line.endswith(\"}\"):\n",
        "                    config = json.load(f)\n",
        "\n",
        "                    raw_data = config.get(\"raw_data\", \"unknown\")\n",
        "                    split_param = config.get(\"split_param\", \"None\")\n",
        "                    resume_as = config.get(\"resume_as\", 1)\n",
        "                    kota = config.get(\"kota\", 1)\n",
        "                    take_up = config.get(\"take_up\", False)\n",
        "\n",
        "                    print(f\"âœ… Config okundu: {raw_data}\")\n",
        "                else:\n",
        "                    raise ValueError(\"Log formatÄ± uymuyor.\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Config hatasÄ±: {e}\")\n",
        "            raw_data = \"result\"; split_param = \"None\"; resume_as = 1; kota = 1; take_up = False\n",
        "    else:\n",
        "        print(\"âŒ Log bulunamadÄ±.\")\n",
        "        raw_data = \"result\"; split_param = \"None\"; resume_as = 1; kota = 1; take_up = False\n",
        "\n",
        "try:\n",
        "    clean_name = os.path.splitext(raw_data)[0]\n",
        "\n",
        "    if take_up:\n",
        "        end_val = int(resume_as) + int(kota) - 1\n",
        "        zip_name = f\"{clean_name}_part{resume_as}_to{end_val}_sp{split_param}\"\n",
        "\n",
        "    else:\n",
        "        zip_name = f\"{clean_name}_part{resume_as}_sp{split_param}\"\n",
        "\n",
        "except NameError:\n",
        "    zip_name = \"colabfold_results_backup\"\n",
        "\n",
        "output_zip_path = f\"/content/{zip_name}\"\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    print(f\"\\nğŸ“¦ '{zip_name}.zip' hazÄ±rlanÄ±yor...\")\n",
        "\n",
        "    shutil.make_archive(output_zip_path, 'zip', results_dir)\n",
        "\n",
        "    print(\"\\nâ¬‡ï¸ Ä°ndirme baÅŸlatÄ±lÄ±yor...\")\n",
        "    files.download(f\"{output_zip_path}.zip\")\n",
        "\n",
        "    print(f\"\\nâœ… Ä°ÅLEM TAMAM: '{zip_name}.zip' indirildi.\")\n",
        "else:\n",
        "    print(f\"âŒ HATA: SonuÃ§ klasÃ¶rÃ¼ yok: {results_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rgdttGmd1bZQ"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1djQJPqLSpxpFeBztgKI2I7uM2l7jj71z",
      "authorship_tag": "ABX9TyO6P5xE7621fj2gZ2CR6MVu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}